name: Reuseable container workflow

env:
  NVCHECKER_VERSION: 2.19
  DFUPDATE_VERSION: 2.2.2
  GH_TOKEN: ${{ github.token }}

# Provide inputs that control image tagging
on:
  workflow_call:
    outputs:
      changed:
        description: Whether any matrix target detected Dockerfile changes or a missing tag
        value: ${{ jobs.collect-metadata.outputs.changed_any }}
      docker_tag:
        description: JSON map of Docker tags keyed by target name
        value: ${{ jobs.collect-metadata.outputs.docker_tags }}
      proposed_tag:
        description: JSON map of proposed tags keyed by target name
        value: ${{ jobs.collect-metadata.outputs.proposed_tags }}
      image:
        description: JSON map of full image references keyed by target name
        value: ${{ jobs.collect-metadata.outputs.images }}
      targets:
        description: JSON array of target metadata (name, workdir, changed, tag_exists, docker_tag, proposed_tag, image)
        value: ${{ jobs.collect-metadata.outputs.targets_json }}
    inputs:
      # Provide a bash command that will create the image tag
      TAG_COMMAND:
        required: false
        type: string
      # Provide a Dockerfile ENV var that will be used for the image tag
      IMAGE_TAG:
        required: false
        type: string
      TARGETS_JSON:
        required: false
        type: string
        description: JSON array of targets; each target should have name, workdir, image_tag, tag_command
      WORKDIR:
        required: false
        type: string
        default: .

jobs:

  # Run a standard build only on PR/MR create
  build-only:
    name: Container build
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    env:
      WORKDIR: ${{ inputs.WORKDIR }}
    steps:
      - uses: actions/checkout@v4
      - run: env | sort
      - run: df -h
      - run: rm -rf /opt/hostedtoolcache
      - run: df -h
      - run: docker build -t "${GITHUB_REPOSITORY}:$(date +%s)" .
        working-directory: ${{ env.WORKDIR }}

  # Run nvchecker/dfupdate, build/push when needed, and package artifacts per target.
  container-update:
    name: Automated container update (${{ matrix.target.name }})
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        target: ${{ fromJson(inputs.TARGETS_JSON != '' && inputs.TARGETS_JSON || format('[{{"name":"{0}","workdir":"{0}","image_tag":"{1}","tag_command":"{2}"}}]', inputs.WORKDIR, inputs.IMAGE_TAG, inputs.TAG_COMMAND)) }}
    env:
      WORKDIR: ${{ matrix.target.workdir }}
      TARGET_NAME: ${{ matrix.target.name }}
    steps:
      - uses: actions/checkout@v4
      - name: Compute target slug
        run: |
          TARGET_SLUG=$(echo "${TARGET_NAME}" | tr '/.' '-')
          echo "TARGET_SLUG=${TARGET_SLUG}" >> "$GITHUB_ENV"
      - run: echo "TAG_COMMAND=${{ matrix.target.tag_command || inputs.TAG_COMMAND }}" >> "$GITHUB_ENV"
      - run: echo "IMAGE_TAG=${{ matrix.target.image_tag || inputs.IMAGE_TAG }}" >> "$GITHUB_ENV"
      - run: env | sort
      - run: df -h
      - run: docker run --rm --name nvchecker --mount "type=bind,source=${PWD},target=/data/" -w /data "snw35/nvchecker:${NVCHECKER_VERSION}" nvchecker -l debug -c nvchecker.toml
        working-directory: ${{ env.WORKDIR }}
      - run: docker run --rm --name dfupdate --mount "type=bind,source=${PWD},target=/data/" -w /data "snw35/dfupdate:${DFUPDATE_VERSION}"
        working-directory: ${{ env.WORKDIR }}
      - name: Sync version snapshot
        run: |
          if [ -f new_ver.json ]; then
            sudo cp new_ver.json old_ver.json
            sudo chown runner:runner old_ver.json
          fi
        working-directory: ${{ env.WORKDIR }}
      - id: check_changes
        run: |
          if [[ $(git -C "$GITHUB_WORKSPACE" status --porcelain | wc -l) -eq 0 ]]; then
            echo "No repository changes detected."
            echo "changed=false" >> "$GITHUB_OUTPUT"
          else
            echo "Repository changes detected."
            echo "changed=true" >> "$GITHUB_OUTPUT"
          fi
      - run: sudo chown runner:runner Dockerfile
        working-directory: ${{ env.WORKDIR }}
      - run: |
          if [ -f new_ver.json ]; then
            sudo chown runner:runner new_ver.json
          fi
        working-directory: ${{ env.WORKDIR }}
      - run: pip3 install dockerfile-parse
      - run: |
          BASE_VERSION=$(python3 - <<'PY'
          from dockerfile_parse import DockerfileParser

          parser = DockerfileParser(path="Dockerfile")
          base = (parser.baseimage or "").strip()
          if not base:
              raise SystemExit("No base image found in Dockerfile")

          image_no_digest = base.split("@", 1)[0]
          slash = image_no_digest.rfind("/")
          colon = image_no_digest.rfind(":")
          tag = image_no_digest[colon + 1 :] if colon > slash else "latest"
          print(tag)
          PY
          )
          echo "BASE_VERSION=${BASE_VERSION}" >> "$GITHUB_ENV"
        working-directory: ${{ env.WORKDIR }}
      - run: |
          # Collect ENV values across all Dockerfile stages (DockerfileParser.envs only returns the final stage).
          ENV_VERSIONS=$(python3 - <<'PY'
          from dockerfile_parse import DockerfileParser
          from dockerfile_parse.util import extract_key_values

          def collect_envs(path="Dockerfile"):
              parser = DockerfileParser(path=path)
              envs = {}
              stage_envs = {}
              stage_args = {}
              global_args = {}
              in_stage = False

              for instruction in parser.structure:
                  name = instruction.get("instruction")
                  value = instruction.get("value")

                  if name == "FROM":
                      in_stage = True
                      stage_envs = {}
                      stage_args = {}
                  elif name == "ARG":
                      for key, val in extract_key_values(False, stage_args, stage_envs, value):
                          if in_stage:
                              if key in global_args:
                                  val = global_args[key]
                              stage_args[key] = val
                          else:
                              global_args[key] = val
                  elif name == "ENV":
                      for key, val in extract_key_values(True, stage_args, stage_envs, value):
                          stage_envs[key] = val
                          envs[key] = val

              return envs

          suffix = "_VERSION"
          versions = [value for key, value in collect_envs().items() if key.endswith(suffix)]
          print("-".join(versions))
          PY
          )
          echo "ENV_VERSIONS=${ENV_VERSIONS}" >> "$GITHUB_ENV"
        working-directory: ${{ env.WORKDIR }}
      - run: |
          if [[ ${{ env.WORKDIR }} == '.' ]]; then
            echo "PROPOSED_TAG=${ENV_VERSIONS}-${BASE_VERSION}" >> "$GITHUB_ENV"
          else
            echo "PROPOSED_TAG=${{ env.WORKDIR }}-${ENV_VERSIONS}-${BASE_VERSION}" >> "$GITHUB_ENV"
          fi
      - run: |
          if [[ -z "${IMAGE_TAG}" ]]; then
            echo "DOCKER_TAG=$PROPOSED_TAG" >> "$GITHUB_ENV"
          else
            IMAGE_TAG_VALUE=$(python3 - "${IMAGE_TAG}" <<'PY'
          import sys
          from dockerfile_parse import DockerfileParser
          from dockerfile_parse.util import extract_key_values

          name = sys.argv[1]

          def collect_envs(path="Dockerfile"):
              parser = DockerfileParser(path=path)
              envs = {}
              stage_envs = {}
              stage_args = {}
              global_args = {}
              in_stage = False

              for instruction in parser.structure:
                  inst = instruction.get("instruction")
                  value = instruction.get("value")

                  if inst == "FROM":
                      in_stage = True
                      stage_envs = {}
                      stage_args = {}
                  elif inst == "ARG":
                      for key, val in extract_key_values(False, stage_args, stage_envs, value):
                          if in_stage:
                              if key in global_args:
                                  val = global_args[key]
                              stage_args[key] = val
                          else:
                              global_args[key] = val
                  elif inst == "ENV":
                      for key, val in extract_key_values(True, stage_args, stage_envs, value):
                          stage_envs[key] = val
                          envs[key] = val

              return envs

          value = collect_envs().get(name)
          if value is None:
              sys.exit(f"Environment variable '{name}' not found in Dockerfile")
          print(value)
          PY
            )
            if [[ ${{ env.WORKDIR }} == '.' ]]; then
              echo "DOCKER_TAG=${IMAGE_TAG_VALUE}" >> "$GITHUB_ENV"
            else
              echo "DOCKER_TAG=${{ env.WORKDIR }}-${IMAGE_TAG_VALUE}" >> "$GITHUB_ENV"
            fi
          fi
        working-directory: ${{ env.WORKDIR }}
      - run: |
          if [[ -n "${TAG_COMMAND}" ]]; then
            if [[ ${{ env.WORKDIR }} == '.' ]]; then
              echo "DOCKER_TAG=$($TAG_COMMAND)" >> "$GITHUB_ENV"
            else
              echo "DOCKER_TAG=${{ env.WORKDIR }}-$($TAG_COMMAND)" >> "$GITHUB_ENV"
            fi
          fi
        working-directory: ${{ env.WORKDIR }}
      - id: check_tags
        run: |
          repo_url="https://${GH_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          if [ "$(git ls-remote --tags "$repo_url" "${DOCKER_TAG}" | wc -l)" -eq 0 ]; then
            echo "Docker tag does not exist on remote."
            echo "tag_exists=false" >> "$GITHUB_OUTPUT"
          else
            echo "Docker tag already exists on remote."
            echo "tag_exists=true" >> "$GITHUB_OUTPUT"
          fi
      - name: Set update variables
        run: |
          {
            echo "CHANGED=${{ steps.check_changes.outputs.changed }}"
            echo "TAG_EXISTS=${{ steps.check_tags.outputs.tag_exists }}"
            echo "IMAGE=${GITHUB_REPOSITORY}:${DOCKER_TAG}"
            echo "TARGET_NAME=${TARGET_NAME}"
          } >> "$GITHUB_ENV"
      - run: env | sort
      - run: echo "${{ secrets.DOCKER_PASSWORD }}" | docker login -u "$GITHUB_REPOSITORY_OWNER" --password-stdin
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: git clone --depth 1 https://github.com/docker-library/official-images.git ~/official-images
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: rm -rf /opt/hostedtoolcache
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: df -h
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: docker build -t "$IMAGE" .
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
        working-directory: ${{ env.WORKDIR }}
      - run: ~/official-images/test/run.sh "$IMAGE" || exit 1;
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: docker push "$IMAGE"
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - run: docker tag "$IMAGE" "${GITHUB_REPOSITORY}:${WORKDIR}"
        if: (env.CHANGED == 'true' || env.TAG_EXISTS == 'false') && env.WORKDIR != '' && env.WORKDIR != '.'
      - run: docker push "${GITHUB_REPOSITORY}:${WORKDIR}"
        if: (env.CHANGED == 'true' || env.TAG_EXISTS == 'false') && env.WORKDIR != '' && env.WORKDIR != '.'
      - run: docker tag "$IMAGE" "${GITHUB_REPOSITORY}:latest"
        if: (env.CHANGED == 'true' || env.TAG_EXISTS == 'false') && (env.WORKDIR == '' || env.WORKDIR == '.')
      - run: docker push "${GITHUB_REPOSITORY}:latest"
        if: (env.CHANGED == 'true' || env.TAG_EXISTS == 'false') && (env.WORKDIR == '' || env.WORKDIR == '.')
      - run: docker images
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
      - name: Stage and package repository changes
        if: env.CHANGED == 'true' || env.TAG_EXISTS == 'false'
        run: |
          git -C "$GITHUB_WORKSPACE" add -A
          if git -C "$GITHUB_WORKSPACE" diff --cached --quiet; then
            echo "No repository changes to package for ${TARGET_NAME}"
          else
            git -C "$GITHUB_WORKSPACE" diff --binary --cached > "/tmp/${TARGET_SLUG}-changes.patch"
          fi
      - name: Refresh changed flag after packaging
        run: |
          if git -C "$GITHUB_WORKSPACE" diff --cached --quiet && git -C "$GITHUB_WORKSPACE" diff --quiet; then
            echo "CHANGED=false" >> "$GITHUB_ENV"
          else
            echo "CHANGED=true" >> "$GITHUB_ENV"
          fi
      - name: Write update metadata
        run: |
          cat > "/tmp/${TARGET_SLUG}-meta.json" <<EOF
          {
            "name": "${TARGET_NAME}",
            "workdir": "${WORKDIR}",
            "changed": "${CHANGED}",
            "tag_exists": "${TAG_EXISTS}",
            "proposed_tag": "${PROPOSED_TAG}",
            "docker_tag": "${DOCKER_TAG}",
            "image": "${IMAGE}"
          }
          EOF
      - uses: actions/upload-artifact@v4
        with:
          name: update-${{ env.TARGET_SLUG }}-${{ github.run_attempt }}
          path: |
            /tmp/${{ env.TARGET_SLUG }}-*
          if-no-files-found: error
          overwrite: true

  collect-metadata:
    name: Collect metadata
    needs: container-update
    if: >
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
      && needs.container-update.result != 'failure'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    outputs:
      changed_any: ${{ steps.collect.outputs.changed_any }}
      docker_tags: ${{ steps.collect.outputs.docker_tags }}
      proposed_tags: ${{ steps.collect.outputs.proposed_tags }}
      images: ${{ steps.collect.outputs.images }}
      targets_json: ${{ steps.collect.outputs.targets_json }}
    steps:
      - uses: actions/download-artifact@v4
        with:
          pattern: update-*-${{ github.run_attempt }}
          path: /tmp/updates
          merge-multiple: true
        continue-on-error: true
      - id: collect
        run: |
          python <<'PY'
          import glob
          import json
          import os

          metas = glob.glob(os.path.join("/tmp/updates", "*-meta.json"))
          targets = []
          for meta_file in metas:
            with open(meta_file, "r", encoding="utf-8") as handle:
              meta = json.load(handle)
            meta["changed"] = str(meta.get("changed", "false")).lower() == "true"
            meta["tag_exists"] = str(meta.get("tag_exists", "false")).lower() == "true"
            targets.append(meta)

          changed_any = any(t["changed"] or not t["tag_exists"] for t in targets)
          docker_tags = {t.get("name"): t.get("docker_tag", "") for t in targets}
          proposed_tags = {t.get("name"): t.get("proposed_tag", "") for t in targets}
          images = {t.get("name"): t.get("image", "") for t in targets}
          tags_to_create = [t.get("proposed_tag") for t in targets if t.get("proposed_tag") and not t["tag_exists"]]

          output = os.environ["GITHUB_OUTPUT"]
          with open(output, "a", encoding="utf-8") as handle:
            handle.write(f"meta_found={str(len(targets) > 0).lower()}\n")
            handle.write(f"changed_any={str(changed_any).lower()}\n")
            handle.write(f"targets_json={json.dumps(targets)}\n")
            handle.write(f"docker_tags={json.dumps(docker_tags)}\n")
            handle.write(f"proposed_tags={json.dumps(proposed_tags)}\n")
            handle.write(f"images={json.dumps(images)}\n")
            handle.write(f"tags_to_create={len(tags_to_create)}\n")
            handle.write(f"tag_names={json.dumps(tags_to_create)}\n")
          PY
